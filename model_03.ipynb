{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== CONFIG ==========\n",
    "DATA_DIR = 'tammathon-task-1\\\\train\\\\train\\\\'\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "N = 100 # how many folders to use (for testing purposes you can set N = 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# ========== STEP 1: LOAD & SPLIT DATA ==========\n",
    "def load_image_paths(data_dir):\n",
    "    dirs = os.listdir(data_dir)\n",
    "    label_names = sorted(dirs)[:N]\n",
    "    print(len(label_names))\n",
    "    \n",
    "    label_to_idx = {label: idx for idx, label in enumerate(label_names)}\n",
    "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "\n",
    "    for i, label_name in enumerate(label_names):\n",
    "        label_path = os.path.join(data_dir, label_name)\n",
    "        image_paths = [os.path.join(label_path, f) for f in sorted(os.listdir(label_path), reverse=False) if f.endswith('.png')]\n",
    "        \n",
    "        if len(image_paths) >= 3:\n",
    "            train_data.extend((img, label_to_idx[label_name]) for img in image_paths[1:])\n",
    "            val_data.append((image_paths[0], label_to_idx[label_name]))\n",
    "        else:\n",
    "            train_data.extend((img, label_to_idx[label_name]) for img in image_paths[:])\n",
    "        \n",
    "        if not i % 10000:\n",
    "            print(i)\n",
    "\n",
    "    return train_data, val_data, label_to_idx, idx_to_label\n",
    "\n",
    "\n",
    "train_data, val_data, label_to_idx, idx_to_label = load_image_paths(DATA_DIR)\n",
    "num_classes = len(label_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 2: DEFINE DATASET ==========\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class CatDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "train_dataset = CatDataset(train_data, transform=transform)\n",
    "val_dataset = CatDataset(val_data, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\z20250128\\myvenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Projects\\z20250128\\myvenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# ========== STEP 3: MODEL WITH EMBEDDINGS ==========\n",
    "\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=512):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()  # remove classification head\n",
    "        self.embedding = nn.Linear(in_features, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.embedding(x)\n",
    "        return F.normalize(x, p=2, dim=1)  # important for cosine similarity\n",
    "\n",
    "model = EmbeddingModel().to(DEVICE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== STEP 4: ARC FACE LOSS ==========\n",
    "\n",
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes, scale=30.0, margin=0.50):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(num_classes, embedding_dim))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, embeddings, labels):\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        W = F.normalize(self.weight, p=2, dim=1)\n",
    "\n",
    "        cosine = F.linear(embeddings, W)\n",
    "        theta = torch.acos(torch.clamp(cosine, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "        target_logits = torch.cos(theta + self.margin)\n",
    "\n",
    "        one_hot = F.one_hot(labels, num_classes=self.num_classes).float().to(DEVICE)\n",
    "        output = cosine * (1 - one_hot) + target_logits * one_hot\n",
    "        return F.cross_entropy(self.scale * output, labels)\n",
    "\n",
    "loss_fn = ArcFaceLoss(embedding_dim=512, num_classes=num_classes).to(DEVICE)\n",
    "optimizer = optim.Adam(list(model.parameters()) + list(loss_fn.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 17.9793\n",
      "Epoch 2/10, Loss: 14.7424\n",
      "Epoch 3/10, Loss: 11.0001\n",
      "Epoch 4/10, Loss: 6.7598\n",
      "Epoch 5/10, Loss: 3.6038\n",
      "Epoch 6/10, Loss: 1.4773\n",
      "Epoch 7/10, Loss: 0.5082\n",
      "Epoch 8/10, Loss: 0.1639\n",
      "Epoch 9/10, Loss: 0.0783\n",
      "Epoch 10/10, Loss: 0.0489\n"
     ]
    }
   ],
   "source": [
    "# ========== STEP 5: TRAINING LOOP ==========\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(images)\n",
    "        loss = loss_fn(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    result_vars = {\n",
    "        'model': model,\n",
    "        'running_loss': running_loss,\n",
    "        'optimizer': optimizer,\n",
    "        }\n",
    "        \n",
    "    with open(f'model_03_result_vars_epoch{epoch+1}.pkl', 'wb') as f:\n",
    "        pickle.dump(result_vars, f)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {running_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 Accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "# ========== STEP 6: KNN VALIDATION ==========\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Get embeddings for train\n",
    "train_embeddings = []\n",
    "train_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in DataLoader(train_dataset, batch_size=BATCH_SIZE):\n",
    "        images = images.to(DEVICE)\n",
    "        emb = model(images)\n",
    "        train_embeddings.append(emb.cpu())\n",
    "        train_labels.extend(labels)\n",
    "\n",
    "train_embeddings = torch.cat(train_embeddings).numpy()\n",
    "train_labels = [idx_to_label[i.item()] for i in train_labels]\n",
    "\n",
    "# Fit KNN (cosine = 1 - similarity)\n",
    "knn = NearestNeighbors(n_neighbors=10, metric='cosine', algorithm='brute')\n",
    "knn.fit(train_embeddings)\n",
    "\n",
    "# Validation with enforced 3 unique nearest neighbors\n",
    "top3_predictions = []\n",
    "true_labels_str = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image, label in val_loader:\n",
    "        image = image.to(DEVICE)\n",
    "        emb = model(image).cpu().numpy()\n",
    "        distances, indices = knn.kneighbors(emb, n_neighbors=20)\n",
    "\n",
    "        seen = set()\n",
    "        preds = []\n",
    "        for idx in indices[0]:\n",
    "            lbl = train_labels[idx]\n",
    "            if lbl not in seen:\n",
    "                preds.append(lbl)\n",
    "                seen.add(lbl)\n",
    "            if len(preds) == 3:\n",
    "                break\n",
    "        \n",
    "        top3_predictions.append(preds)\n",
    "        true_labels_str.append(idx_to_label[label.item()])\n",
    "\n",
    "# Accuracy\n",
    "correct = sum(t in p for t, p in zip(true_labels_str, top3_predictions))\n",
    "print(f\"Top-3 Accuracy: {correct / len(val_data):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
